{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bc537398-0823-41a7-b844-c5fe4c262634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"heart.csv\")\n",
    "\n",
    "data.fillna(data.median(numeric_only=True), inplace=True)\n",
    "\n",
    "le = LabelEncoder()\n",
    "data['Sex'] = le.fit_transform(data['Sex']) \n",
    "data['ChestPainType'] = le.fit_transform(data['ChestPainType'])\n",
    "data['RestingECG'] = le.fit_transform(data['RestingECG'])\n",
    "data['ExerciseAngina'] = le.fit_transform(data['ExerciseAngina'])\n",
    "data['ST_Slope'] = le.fit_transform(data['ST_Slope'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "num_features = ['Age', 'Cholesterol', 'RestingBP',\"MaxHR\"] \n",
    "scaled_features = scaler.fit_transform(data[num_features])\n",
    "data[num_features] = scaled_features\n",
    "\n",
    "X = data.drop(columns=[\"HeartDisease\"])\n",
    "y = data[\"HeartDisease\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"MLP Classifier\": MLPClassifier(random_state=42, max_iter=1000),\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    \"Logistic Regression\": {\n",
    "        \"C\": [0.1, 1, 10],\n",
    "        \"penalty\": [\"l2\"],\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"n_estimators\": [100, 200],\n",
    "        \"max_depth\": [5, 10, 20],\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        \"n_estimators\": [100, 200],\n",
    "        \"learning_rate\": [0.01, 0.1],\n",
    "        \"max_depth\": [3, 5],\n",
    "    },\n",
    "    \"MLP Classifier\": {\n",
    "        \"hidden_layer_sizes\": [(20,), (40,), (10,)],\n",
    "        \"activation\": [\"relu\", \"tanh\",],\n",
    "        \"alpha\": [0.0001, 0.001, 0.01],\n",
    "    },\n",
    "}\n",
    "\n",
    "best_models = {\n",
    "    \"model_name\": [],\n",
    "    \"model\" : [],\n",
    "    \"best_params\": [],\n",
    "    \"avg_cross_val_f1 (%)\": [],\n",
    "    \"test_f1 (%)\": [],\n",
    "}\n",
    "\n",
    "for i, (model_name, model) in enumerate(models.items()):\n",
    "    print(f\"Running model : {i+1}\")\n",
    "    grid_search = GridSearchCV(model, param_grids[model_name], scoring=\"f1\", cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_models['model_name'].append(model_name)\n",
    "    best_models['model'].append(grid_search.best_estimator_)\n",
    "    best_models['best_params'].append(grid_search.best_params_)\n",
    "\n",
    "print(\"performing cross validation ...\")\n",
    "for model_name, model in zip(best_models['model_name'],best_models['model']):\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, scoring=\"f1\", cv=5)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    best_models['avg_cross_val_f1 (%)'].append(round(cv_scores.mean() * 100, 2))\n",
    "    best_models['test_f1 (%)'].append(round(f1_score(y_test, y_pred) * 100, 2))\n",
    "\n",
    "best_models = pd.DataFrame(best_models)\n",
    "best_models = best_models.sort_values(by = 'avg_cross_val_f1 (%)', ascending=False,ignore_index=True)\n",
    "\n",
    "\n",
    "final_model = best_models['model'][0]\n",
    "\n",
    "\n",
    "import joblib\n",
    "\n",
    "joblib.dump(final_model, \"model.pkl\")\n",
    "joblib.dump(scaler, 'scalar.pkl')\n",
    "print(\"Model and Scaler Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd83cb5-499e-420b-bac6-5219eaae7472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29e8ae4-7dd3-49a0-8246-c0adf2e6fbc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
